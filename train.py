# -*- coding: utf-8 -*-
"""classification-MTGNN-attn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oglsWX-C5i7H-_l3h8-D8nW_VVbdf67u

# Direction of price movement for crude oil and preciouse metal

## Set Up
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.sparse as sp
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm

from modelArchitecture import MTGNN_Att
from helperFunctions import *

torch.manual_seed(123)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


"""### Define Parameters ans Split Data"""

in_dim = 1
out_dim = 1
n_out_seq = 1
num_epochs = 40
clip = 5

# model parameters
kernel_set = [2, 3, 6, 7]
gcn_depth = 2

## valid_acc = 0.80346
n_in_seq = 20 # number of historical time steps to consider
batch_size = 32
learning_rate = 0.005
heads = 4
layers = 2 # number of MTGNNLayer
node_emb_dim = 32
out = 32
skip = 16
conv_res = 64
subgraph_size = 15
reg_penalty = 0.0001
batch_size = 16
dropout= 0.5


"""## Model training"""

model = MTGNN_Att(gcn_true= True, build_adj= True, gcn_depth= gcn_depth,
              num_nodes= num_nodes, kernel_set= kernel_set, kernel_size=7,
              dropout= dropout, subgraph_size= subgraph_size, node_dim= node_emb_dim,
              dilation_exponential=2, conv_channels= conv_res,
              residual_channels = conv_res, skip_channels= skip, end_channels= out,
              seq_length= n_in_seq, in_dim= 1, out_dim= n_out_seq, layers= layers,
              propalpha= 0.05, tanhalpha= 3, layer_norm_affline= True, xd= None).to(device)

# define loss function
loss = nn.BCEWithLogitsLoss()
# define optimizer
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate,
                             weight_decay = reg_penalty)

max_val_acc = 0
train_losses = []
val_losses = []

for epoch in tqdm(range(1, num_epochs + 1), desc = 'Epoch', position = 0):
  l_sum, n, correct = 0.0, 0, 0

  model.train()

  for x, y in tqdm(train_iter, desc = 'Batch', position = 0):
    # Input shape: (batch_size, seq_len, num_nodes, in_dim)
    x = x.permute(0, 3, 2, 1).to(device)
    y = y.to(device)

    # y_pred shape (batch_size, num_nodes)
    y_pred, A_tilde = model(x)
    y_pred = y_pred.view(len(x),-1)

    l = loss(y_pred, y)
    # backpropogation
    optimizer.zero_grad()
    l.backward()
    # gradient clipping
    if clip is not None:
      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()

    n += y.shape[0]
    l_sum += l.item() * y.shape[0]
    # y_pred_binary = torch.round(torch.sigmoid(y_pred))
    y_pred_binary = (y_pred > 0).float()
    for i in range(y.shape[0]):
      correct_nodes = (y_pred_binary[i] == y[i]).sum().item()
      correct += correct_nodes

  acc = correct/(n*num_nodes)
  train_losses.append(l_sum/n)


  # compute validation accuracy
  val_acc , val_loss = evaluate_model(model, loss, val_iter, device)
  val_losses.append(val_loss)
  # save the model if the validation loss is less than the current min validation loss
  if val_acc > max_val_acc:
      max_val_acc = val_acc
      torch.save(model.state_dict(), model_save_path)
  print(f"epoch: {epoch}, train acc: {acc:.5f}, Valid acc: {val_acc:.5f}")

torch.save(A_tilde, adj_save_path)
adjacency_matrix = torch.load(adj_save_path)


# find the number of parameters in each module
num_of_prams = count_parameters(model) #  398209

loss_plot = plot_losses(train_losses, val_losses)
